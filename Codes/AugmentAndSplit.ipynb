{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4000,
     "status": "ok",
     "timestamp": 1739045594974,
     "user": {
      "displayName": "Lakruwan Jayathissa",
      "userId": "06077991306446010250"
     },
     "user_tz": -330
    },
    "id": "Sg7bGKFe9GDi",
    "outputId": "e216a41b-29cd-4c8c-8ecb-84078396ad35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1OJxr_r9Qa2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 628,
     "status": "ok",
     "timestamp": 1739045885477,
     "user": {
      "displayName": "Lakruwan Jayathissa",
      "userId": "06077991306446010250"
     },
     "user_tz": -330
    },
    "id": "elqZjp1nAIS4",
    "outputId": "14929179-fc22-40c1-cc06-b01c0e8b6038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1:\n",
      "230\n",
      "v2:\n",
      "230\n",
      "v3:\n",
      "230\n",
      "v4:\n",
      "230\n"
     ]
    }
   ],
   "source": [
    "# Define paths in Google Drive\n",
    "input_dir = \"/content/drive/My Drive/Research/DatasetCropped\"  # Path to cropped buds\n",
    "output_dir = \"/content/drive/My Drive/Research/DatasetAugmented\"  # Path to save augmented dataset\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#print image counts\n",
    "!echo \"v1:\" && find \"/content/drive/My Drive/Research/DatasetCropped/v1\" -type f | wc -l\n",
    "!echo \"v2:\" && find \"/content/drive/My Drive/Research/DatasetCropped/v2\" -type f | wc -l\n",
    "!echo \"v3:\" && find \"/content/drive/My Drive/Research/DatasetCropped/v3\" -type f | wc -l\n",
    "!echo \"v4:\" && find \"/content/drive/My Drive/Research/DatasetCropped/v4\" -type f | wc -l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739045889162,
     "user": {
      "displayName": "Lakruwan Jayathissa",
      "userId": "06077991306446010250"
     },
     "user_tz": -330
    },
    "id": "uhczU8VoAKP6"
   },
   "outputs": [],
   "source": [
    "# Augmentation pipeline\n",
    "augmentations = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),  # Flip horizontally\n",
    "    A.RandomBrightnessContrast(p=0.3),  # Adjust brightness/contrast\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.3),  # Add noise\n",
    "    A.HueSaturationValue(p=0.3),  # Slight color shift\n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.3),  # Add blur\n",
    "    A.Sharpen(p=0.3),  # Sharpen edges\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 161186,
     "status": "ok",
     "timestamp": 1739046052930,
     "user": {
      "displayName": "Lakruwan Jayathissa",
      "userId": "06077991306446010250"
     },
     "user_tz": -330
    },
    "id": "QQJQfD_FANEs",
    "outputId": "633fba2c-2b26-4cdd-b2c7-e5d878bca2a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:37<00:00,  6.08it/s]\n",
      "100%|██████████| 230/230 [00:38<00:00,  5.96it/s]\n",
      "100%|██████████| 230/230 [00:42<00:00,  5.44it/s]\n",
      "100%|██████████| 230/230 [00:41<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation completed!\n",
      "v1:\n",
      "1380\n",
      "v2:\n",
      "1380\n",
      "v3:\n",
      "1380\n",
      "v4:\n",
      "1380\n"
     ]
    }
   ],
   "source": [
    "# Augment dataset\n",
    "num_augments = 5  # Number of augmented versions per image\n",
    "for subfolder in os.listdir(input_dir):\n",
    "    subfolder_path = os.path.join(input_dir, subfolder)\n",
    "    output_subfolder_path = os.path.join(output_dir, subfolder)\n",
    "    os.makedirs(output_subfolder_path, exist_ok=True)\n",
    "\n",
    "    for img_name in tqdm(os.listdir(subfolder_path)):\n",
    "        img_path = os.path.join(subfolder_path, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "\n",
    "        # Save original\n",
    "        shutil.copy(img_path, os.path.join(output_subfolder_path, img_name))\n",
    "\n",
    "        # Generate augmented images\n",
    "        for i in range(num_augments):\n",
    "            augmented = augmentations(image=image)['image']\n",
    "            aug_img_name = f\"{os.path.splitext(img_name)[0]}_aug{i}.jpg\"\n",
    "            cv2.imwrite(os.path.join(output_subfolder_path, aug_img_name), augmented)\n",
    "\n",
    "print(\"Data augmentation completed!\")\n",
    "!echo \"v1:\" && find \"/content/drive/My Drive/Research/DatasetAugmented/v1\" -type f | wc -l\n",
    "!echo \"v2:\" && find \"/content/drive/My Drive/Research/DatasetAugmented/v2\" -type f | wc -l\n",
    "!echo \"v3:\" && find \"/content/drive/My Drive/Research/DatasetAugmented/v3\" -type f | wc -l\n",
    "!echo \"v4:\" && find \"/content/drive/My Drive/Research/DatasetAugmented/v4\" -type f | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226816,
     "status": "ok",
     "timestamp": 1739047034492,
     "user": {
      "displayName": "Lakruwan Jayathissa",
      "userId": "06077991306446010250"
     },
     "user_tz": -330
    },
    "id": "9FCzI9I-AQPs",
    "outputId": "3bd59c46-bf37-4768-9157-cfbe12765a6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into train/val/test\n",
    "base_dir = \"/content/drive/My Drive/Research/DatasetAugmented\"\n",
    "output_dir = \"/content/drive/My Drive/Research/DatasetAugmented_Split\"\n",
    "\n",
    "train_dir = os.path.join(output_dir, \"train\")\n",
    "val_dir = os.path.join(output_dir, \"val\")\n",
    "test_dir = os.path.join(output_dir, \"test\")\n",
    "\n",
    "# Create train/val/test folders\n",
    "for split_dir in [train_dir, val_dir, test_dir]:\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "# Get subfolders (v1, v2, v3, v4)\n",
    "subfolders = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    subfolder_path = os.path.join(base_dir, subfolder)\n",
    "    images = os.listdir(subfolder_path)\n",
    "    random.shuffle(images)\n",
    "\n",
    "    train_split = int(len(images) * 0.7)\n",
    "    val_split = int(len(images) * 0.85)\n",
    "\n",
    "    # Create class-wise folders inside train/val/test\n",
    "    for split_name, split_range in zip([\"train\", \"val\", \"test\"], [(0, train_split), (train_split, val_split), (val_split, len(images))]):\n",
    "        split_subfolder_path = os.path.join(output_dir, split_name, subfolder)\n",
    "        os.makedirs(split_subfolder_path, exist_ok=True)\n",
    "\n",
    "        for i in range(*split_range):\n",
    "            src_path = os.path.join(subfolder_path, images[i])\n",
    "            dst_path = os.path.join(split_subfolder_path, images[i])\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "print(\"Dataset split completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1638,
     "status": "ok",
     "timestamp": 1739047188701,
     "user": {
      "displayName": "Lakruwan Jayathissa",
      "userId": "06077991306446010250"
     },
     "user_tz": -330
    },
    "id": "-n6SpcjVRDVV",
    "outputId": "9ee7f6e3-24cd-4c47-a42f-e0770432c221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "v1:\n",
      "965\n",
      "v2:\n",
      "965\n",
      "v3:\n",
      "965\n",
      "v4:\n",
      "965\n",
      "Validation Set:\n",
      "v1:\n",
      "208\n",
      "v2:\n",
      "208\n",
      "v3:\n",
      "208\n",
      "v4:\n",
      "208\n",
      "Test Set:\n",
      "v1:\n",
      "207\n",
      "v2:\n",
      "207\n",
      "v3:\n",
      "207\n",
      "v4:\n",
      "207\n"
     ]
    }
   ],
   "source": [
    "!echo \"Train Set:\"\n",
    "!echo \"v1:\" && find \"/content/drive/My Drive/Research/DatasetAugmented_Split/train/v1\" -type f | wc -l\n",
    "!echo \"v2:\" && find \"/content/drive/My Drive/Research/DatasetAugmented_Split/train/v2\" -type f | wc -l\n",
    "!echo \"v3:\" && find \"/content/drive/My Drive/Research/DatasetAugmented_Split/train/v3\" -type f | wc -l\n",
    "!echo \"v4:\" && find \"/content/drive/My Drive/Research/DatasetAugmented_Split/train/v4\" -type f | wc -l\n",
    "\n",
    "!echo \"Validation Set:\"\n",
    "!echo \"v1:\" && find \"/content/drive/My Drive/Research/DatasetAugmented_Split/val/v1\" -type f | wc -l\n",
    "!echo \"v2:\" && find \"/content/drive/My Drive/Research/DatasetAugmented_Split/val/v2\" -type f | wc -l\n",
    "!echo \"v3:\" && find \"/content/drive/My Drive/Research/DatasetAugmented_Split/val/v3\" -type f | wc -l\n",
    "!echo \"v4:\" && find \"/content/drive/My Drive/Research/DatasetAugmented_Split/val/v4\" -type f | wc -l\n",
    "\n",
    "!echo \"Test Set:\"\n",
    "!echo \"v1:\" && find \"/content/drive/My Drive/Research/DatasetAugmented_Split/test/v1\" -type f | wc -l\n",
    "!echo \"v2:\" && find \"/content/drive/My Drive/Research/DatasetAugmented_Split/test/v2\" -type f | wc -l\n",
    "!echo \"v3:\" && find \"/content/drive/My Drive/Research/DatasetAugmented_Split/test/v3\" -type f | wc -l\n",
    "!echo \"v4:\" && find \"/content/drive/My Drive/Research/DatasetAugmented_Split/test/v4\" -type f | wc -l"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6sp1XW66Y36x0mva6Weq8",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
